{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here I will be trying to classify sentences according to their labels using CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from urllib.request import urlretrieve\n",
    "seed=1234\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading data: the data is composed of questions as input and their type as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='http://cogcomp.org/Data/QA/QC/'\n",
    "dir_name='data'\n",
    "def download_data(dir_name,filename):\n",
    "    os.makedirs(dir_name,exist_ok=True)\n",
    "    if not os.path.exists(os.path.join(dir_name,filename)):\n",
    "        filepath,_=urlretrieve(url+filename,os.path.join(dir_name,filename))\n",
    "    else:\n",
    "        filepath=os.path.join(dir_name,filename)\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename=download_data(dir_name,'train_5500.label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename=download_data(dir_name,'TREC_10.label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading and preprocessing data: load the text and for each example extract question,category and subcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    '''\n",
    "    Read data from a file with given filename\n",
    "    Return a list of strings\n",
    "    '''\n",
    "    # hold question, cateegory and subcategory\n",
    "    questions,categories,sub_categories=[],[],[]\n",
    "    with open(filename,'r',encoding='latin-1') as f:\n",
    "        # read each line\n",
    "        # Each string has format <cat>:<sub cat> <question>\n",
    "        # Split by : to separate cat and (sub_cat + question)\n",
    "        for row in f:\n",
    "            row_str=row.split(\":\")\n",
    "            cat,sub_cat_and_question=row_str[0],row_str[1]\n",
    "            tokens=sub_cat_and_question.split(' ')\n",
    "            sub_cat,question=tokens[0],' '.join(tokens[1:])\n",
    "            questions.append(question.lower().strip())\n",
    "            categories.append(cat)\n",
    "            sub_categories.append(sub_cat)\n",
    "    return questions,categories,sub_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions,train_categories,train_sub_categories=read_data(train_filename)\n",
    "test_questions,test_categories,test_sub_categories=read_data(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train / test data to pandas dataframe\n",
    "train_df=pd.DataFrame({'question':train_questions,'category':train_categories,'sub_category':train_sub_categories})\n",
    "test_df=pd.DataFrame({'question':test_questions,'category':test_categories,'sub_category':test_sub_categories})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how did serfdom develop in and then leave russ...</td>\n",
       "      <td>DESC</td>\n",
       "      <td>manner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what films featured the character popeye doyle ?</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>cremat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how can i find a list of celebrities ' real na...</td>\n",
       "      <td>DESC</td>\n",
       "      <td>manner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what fowl grabs the spotlight after the chines...</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>animal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is the full form of .com ?</td>\n",
       "      <td>ABBR</td>\n",
       "      <td>exp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question category sub_category\n",
       "0  how did serfdom develop in and then leave russ...     DESC       manner\n",
       "1   what films featured the character popeye doyle ?     ENTY       cremat\n",
       "2  how can i find a list of celebrities ' real na...     DESC       manner\n",
       "3  what fowl grabs the spotlight after the chines...     ENTY       animal\n",
       "4                    what is the full form of .com ?     ABBR          exp"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "train_df=train_df.sample(frac=1,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label-> ID mapping:{'HUM': 0, 'DESC': 1, 'LOC': 2, 'ENTY': 3, 'NUM': 4, 'ABBR': 5}\n"
     ]
    }
   ],
   "source": [
    "# Converting string label to integer id\n",
    "unique_cats=train_df.category.unique()\n",
    "labels_map=dict(zip(unique_cats,np.arange(unique_cats.shape[0])))\n",
    "print(f\"Label-> ID mapping:{labels_map}\")\n",
    "n_classes=len(labels_map)\n",
    "train_df['category']=train_df['category'].map(labels_map)\n",
    "test_df['category']=test_df['category'].map(labels_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>who sings the song `` drink to me with thine e...</td>\n",
       "      <td>0</td>\n",
       "      <td>ind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>what are the lyrics to the star spangled banner ?</td>\n",
       "      <td>1</td>\n",
       "      <td>desc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927</th>\n",
       "      <td>which latin american country is the largest ?</td>\n",
       "      <td>2</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>where is logan airport ?</td>\n",
       "      <td>2</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4502</th>\n",
       "      <td>what does larry king do for a living ?</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  category sub_category\n",
       "672   who sings the song `` drink to me with thine e...         0          ind\n",
       "2518  what are the lyrics to the star spangled banner ?         1         desc\n",
       "2927      which latin american country is the largest ?         2      country\n",
       "971                            where is logan airport ?         2        other\n",
       "4502             what does larry king do for a living ?         0        title"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and val sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df,valid_df=train_test_split(train_df,test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a tokenizer that maps words to numerical IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:7895\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df['question'].tolist())\n",
    "# compute vocab size\n",
    "n_vocab=len(tokenizer.index_word)+1\n",
    "print(f\"Vocab size:{n_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all of the train, validation, and test inputs to sequences of word IDs.\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df[\"question\"].tolist())\n",
    "train_labels = train_df[\"category\"].values\n",
    "valid_sequences = tokenizer.texts_to_sequences(valid_df[\"question\"].tolist())\n",
    "valid_labels = valid_df[\"category\"].values\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df[\"question\"].tolist())\n",
    "test_labels = test_df[\"category\"].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the model , questions are feeded in batches and its very unlikely that all questions have same number of tokens, if all questions dont have same no. of tokens, a tensor cannot be formed so shorter sequences need to to pad and sequence longer than specified length need to be truncated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=22\n",
    "# pad short sequence and truncate longer\n",
    "preprocessed_train_sequences=tf.keras.preprocessing.sequence.pad_sequences(train_sequences,maxlen=max_seq_length,padding='post',\n",
    "truncating='post')\n",
    "preprocessed_valid_sequences=tf.keras.preprocessing.sequence.pad_sequences(valid_sequences,maxlen=max_seq_length,padding='post',\n",
    "truncating='post')\n",
    "preprocessed_test_sequences=tf.keras.preprocessing.sequence.pad_sequences(test_sequences,maxlen=max_seq_length,padding='post',\n",
    "truncating='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple CNN to classify sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 22)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 22, 64)       505280      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 22, 100)      19300       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 22, 100)      25700       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 22, 100)      32100       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 22, 300)      0           conv1d[0][0]                     \n",
      "                                                                 conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1, 300)       0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 300)          0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 6)            1806        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 584,186\n",
      "Trainable params: 584,186\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "K.clear_session()\n",
    "word_id_inputs=layers.Input(shape=(max_seq_length,),dtype='int32')\n",
    "embedding_out=layers.Embedding(input_dim=n_vocab,output_dim=64)(word_id_inputs)\n",
    "conv1_1=layers.Conv1D(100,kernel_size=3,strides=1,padding='same',activation='relu')(embedding_out)\n",
    "conv1_2=layers.Conv1D(100,kernel_size=4,strides=1,padding='same',activation='relu')(embedding_out)\n",
    "conv1_3=layers.Conv1D(100,kernel_size=5,strides=1,padding='same',activation='relu')(embedding_out)\n",
    "# concatenate to produce a single tensor\n",
    "conv_out=layers.Concatenate(axis=-1)([conv1_1,conv1_2,conv1_3])\n",
    "# applying pooling layer - max pooling over sequence lenth\n",
    "# in other words, each feature map results in a single output\n",
    "pool_over_time_out=layers.MaxPool1D(pool_size=max_seq_length,padding='valid')(conv_out)\n",
    "# flatten \n",
    "flatten_out=layers.Flatten()(pool_over_time_out)\n",
    "# final output\n",
    "out=layers.Dense(n_classes,activation='softmax',kernel_regularizer=regularizers.l2(0.001))(flatten_out)\n",
    "# define the model\n",
    "cnn_model=Model(inputs=word_id_inputs,outputs=out)\n",
    "# compile the model\n",
    "cnn_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model- using callback for improved performance - use decaying learning rate.The idea is to reduce the learning rate (by some fraction) whenever the model has stopped to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "39/39 [==============================] - 5s 44ms/step - loss: 1.6281 - accuracy: 0.3755 - val_loss: 1.4152 - val_accuracy: 0.5256\n",
      "Epoch 2/25\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 1.1129 - accuracy: 0.6549 - val_loss: 0.8553 - val_accuracy: 0.7289\n",
      "Epoch 3/25\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.6034 - accuracy: 0.8316 - val_loss: 0.5719 - val_accuracy: 0.8223\n",
      "Epoch 4/25\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.3238 - accuracy: 0.9264 - val_loss: 0.4555 - val_accuracy: 0.8571\n",
      "Epoch 5/25\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1810 - accuracy: 0.9678 - val_loss: 0.4094 - val_accuracy: 0.8590\n",
      "Epoch 6/25\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1109 - accuracy: 0.9865 - val_loss: 0.3940 - val_accuracy: 0.8736\n",
      "Epoch 7/25\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0791 - accuracy: 0.9935 - val_loss: 0.3905 - val_accuracy: 0.8846\n",
      "Epoch 8/25\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0619 - accuracy: 0.9971 - val_loss: 0.3901 - val_accuracy: 0.8938\n",
      "Epoch 9/25\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0533 - accuracy: 0.9988 - val_loss: 0.3946 - val_accuracy: 0.8883\n",
      "Epoch 10/25\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0477 - accuracy: 0.9994 - val_loss: 0.3996 - val_accuracy: 0.8956\n",
      "Epoch 11/25\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0439 - accuracy: 0.9996 - val_loss: 0.4014 - val_accuracy: 0.8956\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/25\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0415 - accuracy: 0.9996 - val_loss: 0.4016 - val_accuracy: 0.8938\n",
      "Epoch 13/25\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0413 - accuracy: 0.9996 - val_loss: 0.4018 - val_accuracy: 0.8938\n",
      "Epoch 14/25\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0410 - accuracy: 0.9996 - val_loss: 0.4018 - val_accuracy: 0.8919\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 15/25\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0408 - accuracy: 0.9996 - val_loss: 0.4018 - val_accuracy: 0.8919\n",
      "Epoch 16/25\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0408 - accuracy: 0.9996 - val_loss: 0.4018 - val_accuracy: 0.8919\n",
      "Epoch 17/25\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0407 - accuracy: 0.9996 - val_loss: 0.4018 - val_accuracy: 0.8919\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 18/25\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0407 - accuracy: 0.9996 - val_loss: 0.4018 - val_accuracy: 0.8919\n",
      "Epoch 19/25\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0407 - accuracy: 0.9996 - val_loss: 0.4018 - val_accuracy: 0.8919\n",
      "Epoch 20/25\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0407 - accuracy: 0.9996 - val_loss: 0.4018 - val_accuracy: 0.8919\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 21/25\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0407 - accuracy: 0.9996 - val_loss: 0.4018 - val_accuracy: 0.8919\n",
      "Epoch 22/25\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0407 - accuracy: 0.9996 - val_loss: 0.4018 - val_accuracy: 0.8919\n",
      "Epoch 23/25\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.0407 - accuracy: 0.9996 - val_loss: 0.4018 - val_accuracy: 0.8919\n",
      "Epoch 24/25\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0407 - accuracy: 0.9996 - val_loss: 0.4018 - val_accuracy: 0.8919\n",
      "Epoch 25/25\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0407 - accuracy: 0.9996 - val_loss: 0.4018 - val_accuracy: 0.8919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a5d49fd580>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# callback\n",
    "lr_reduce_callback=tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=3,verbose=1,\n",
    "mode='auto',min_delta=0.0001,min_lr=0.000001)\n",
    "# train \n",
    "cnn_model.fit(preprocessed_train_sequences,train_labels,\n",
    "validation_data=(preprocessed_valid_sequences,valid_labels),batch_size=128,epochs=25,callbacks=[lr_reduce_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3768 - accuracy: 0.9020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.3767916262149811, 'accuracy': 0.9020000100135803}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the model on test set\n",
    "cnn_model.evaluate(preprocessed_test_sequences,test_labels,return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da7773864c198c8559e499b8a6d42753464881661d1a635729c4702e1dcc7c46"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
